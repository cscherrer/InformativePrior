<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3PYLMFWW0P"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    
    gtag('config', 'G-3PYLMFWW0P');
  </script>
  <meta charset="UTF-8">
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Rosario&display=swap" rel="stylesheet"> 
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/minimal-mistakes.css">
<link rel="stylesheet" href="/css/adjust.css">
<link rel="icon" href="/assets/favicon.png">
<!--[if IE ]>
<style>
  /* old IE unsupported flexbox fixes */
  .greedy-nav .site-title {
    padding-right: 3em;
  }
  .greedy-nav button {
    position: absolute;
    top: 0;
    right: 0;
    height: 100%;
  }
</style>
<![endif]-->

   <title>Measure Theory for Probabilistic Modeling</title>  

  <!-- end custom head snippets -->
</head>
<body class="layout--single">
  <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Informative Prior</a>
        <ul class="visible-links">
          <li class="masthead__menu-item"><a href="/about-us/" >About Us</a></li>
          <li class="masthead__menu-item"><a href="/blog/" >Blog</a></li>
        </ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

  <div class="initial-content">
    <div id="main" role="main">
      <div class="sidebar sticky">
        <div itemscope itemtype="https://schema.org/Person">
          <div class="author__avatar">
            <img src="/assets/chad-avatar.jpg" alt="Chad Scherrer" itemprop="image">
          </div>
          <div class="author__content">
            <h3 class="author__name" itemprop="name">Chad Scherrer</h3>
            <p class="author__bio" itemprop="description">Founder</p>
          </div>
          <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>
            <ul class="author__urls social-icons">
              <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
                <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seattle, WA</span></li>
              <li><a href="https://twitter.com/ChadScherrer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
              <li><a href="https://github.com/cscherrer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
            </ul>
          </div>
        </div>
      </div>


<!-- Content appended here -->
<div class="franklin-content">
<h1 id="title"><a href="#title" class="header-anchor">Measure Theory for Probabilistic Modeling</a></h1>
<p><em>January 29, 2021</em></p>
<p><em>Modern probabilistic modeling puts strong demands on the interface and implementation of libraries for probability distributions. MeasureTheory.jl is an effort to address limitations of existing libraries. In this post, we&#39;ll motivate the need for a new library and give an overview of the approach and its benefits.</em></p>
<p><strong>Contents</strong></p>
<div class="franklin-toc"><ol><li><a href="#introduction">Introduction</a></li><li><a href="#example_the_normal_distribution">Example: The Normal Distribution</a></li><li><a href="#normalization_factors">Normalization Factors</a></li><li><a href="#the_normal_measure_in_four_lines_of_code">The Normal Measure, in Four Lines of Code</a><ol><li><a href="#measure_normalμσ_1sqrt2π_lebesgueℝ"><code>@measure Normal&#40;μ,σ&#41; ≪ &#40;1/sqrt2π&#41; * Lebesgue&#40;ℝ&#41;</code></a></li><li><a href="#logdensitydnormal_x_-_x2_2"><code>logdensity&#40;d::Normal&#123;&#40;&#41;&#125; , x&#41; &#61; - x^2 / 2</code></a></li><li><a href="#baserandrngrandomabstractrng_ttype_dnormal_randnrng_t"><code>Base.rand&#40;rng::Random.AbstractRNG, T::Type, d::Normal&#123;&#40;&#41;&#125;&#41; &#61; randn&#40;rng, T&#41;</code></a></li><li><a href="#μσ_methods_normal"><code>@μσ_methods Normal&#40;&#41;</code></a></li></ol></li><li><a href="#performance">Performance</a></li><li><a href="#final_notes">Final Notes</a></li></ol></div>
<h2 id="introduction"><a href="#introduction" class="header-anchor">Introduction</a></h2>
<p>Say you&#39;re working on a problem that requires some distribution computations. Probability density, tail probabilities, that sort of thing. So you find a library that&#39;s supposed to be good for this sort of thing, and get to it.</p>
<p>Along the way, there are bound to be some hiccups. Maybe you need to call <code>pmf</code> for discrete distributions and <code>pdf</code> for continuous ones. Or maybe the library has bounds checking for some distributions but not others, so you have to try to remember which is which.</p>
<p>This may be fine for writing this sort of thing &quot;by hand&quot;. Probabilistic programming goes a step beyond this, taking a high-level representation of a model and automating much of the process.</p>
<p>That&#39;s great, but this automation comes at a price. Any quirks of the interface suddenly become painful sources of bugs, and take lots of case-by-case analysis to work around. Worse, it&#39;s not a one-time cost; any new distributions will require associated code to know how to call them. The whole thing becomes a bloated mess.</p>
<p>This calls for a library with goals like</p>
<ul>
<li><p>Creating new distributions should be straightforward,</p>
</li>
<li><p>Calling conventions should be consistent across distributions,</p>
</li>
<li><p>Types should be used for dispatch, and not as a restriction or documentation, and</p>
</li>
<li><p>When possible &#40;it usually is&#41;, log-densities should be algebraic expressions with no non-trivial control flow.</p>
</li>
</ul>
<p>While we&#39;re at it, we&#39;ll address some issues with working in terms of distributions <em>at all</em>, and see some benefits of generalizing just a bit to instead work in terms of <em>measures</em>.</p>
<p>First, let&#39;s make things a little more concrete.</p>
<h2 id="example_the_normal_distribution"><a href="#example_the_normal_distribution" class="header-anchor">Example: The Normal Distribution</a></h2>
<p><a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> is a popular Julia library for working with probability distributions. At ths time I&#39;m writing this, its GitHub repository has 625 stars and 305 forks. It&#39;s highly visible, and many packages have it as a dependency. This also means changing it in any way can be very difficult.</p>
<p>Here&#39;s the <a href="https://github.com/JuliaStats/Distributions.jl/blob/master/src/univariate/continuous/normal.jl">Distributions implementation</a> of <code>Normal</code>:</p>
<pre><code class="language-julia">struct Normal&#123;T&lt;:Real&#125; &lt;: ContinuousUnivariateDistribution
    μ::T
    σ::T
    Normal&#123;T&#125;&#40;µ::T, σ::T&#41; where &#123;T&lt;:Real&#125; &#61; new&#123;T&#125;&#40;µ, σ&#41;
end

function Normal&#40;μ::T, σ::T; check_args&#61;true&#41; where &#123;T &lt;: Real&#125;
    check_args &amp;&amp; @check_args&#40;Normal, σ &gt;&#61; zero&#40;σ&#41;&#41;
    return Normal&#123;T&#125;&#40;μ, σ&#41;
end</code></pre>
<p>See the <code>T &lt;: Real</code> sprinkled around? In <code>SymbolicUtils</code>, a symbolic representation of, say, a <code>Float64</code> would have type <code>Symbolic&#123;Float64&#125;</code>. But it&#39;s not <code>&lt;: Real</code>, so this is a real headache &#40;no pun intended&#41;.</p>
<p>There&#39;s also that <code>check_args</code>. This makes sure we don&#39;t end up with a negative standard deviation. And yes, it also causes problems for symbolic values, and has some small performance cost. But you can disable it, just by calling with <code>check_args&#61;false</code>.</p>
<p>The bigger problem with this is that it&#39;s not consistent&#33; Sure, you can call <code>Normal</code> with <code>check_args&#61;false</code>. But for some other distributions, this would throw an error. So you end up needing a lookup table specifying how each distribution needs to be called. </p>
<p>Anyway, checking the arguments isn&#39;t necessary&#33; Any call to <code>pdf</code> or <code>logpdf</code> will need to compute <code>log&#40;σ&#41;</code>, which will throw an error if <code>σ &lt; 0</code>.</p>
<p>Moving on, let&#39;s look at the implementation of <code>logpdf</code>:</p>
<pre><code class="language-julia">function logpdf&#40;d::Normal, x::Real&#41;
    μ, σ &#61; d.μ, d.σ
    if iszero&#40;d.σ&#41;
        if x &#61;&#61; μ
            z &#61; zval&#40;Normal&#40;μ, one&#40;σ&#41;&#41;, x&#41;
        else
            z &#61; zval&#40;d, x&#41;
            σ &#61; one&#40;σ&#41;
        end
    else
        z &#61; zval&#40;Normal&#40;μ, σ&#41;, x&#41;
    end
    return _normlogpdf&#40;z&#41; - log&#40;σ&#41;
end

_normlogpdf&#40;z::Real&#41; &#61; -&#40;abs2&#40;z&#41; &#43; log2π&#41;/2</code></pre>
<p>In addition to the <code>::Real</code> type constraint, there&#39;s some overhead for checking against special cases. As before, the issues are incompatibility with symbolic types, and some small overhead for the extra control flow.</p>
<h2 id="normalization_factors"><a href="#normalization_factors" class="header-anchor">Normalization Factors</a></h2>
<p>It&#39;s also worth noting the <code>-log2π/2</code> term in <code>_normlogpdf</code>. This is of course correct; it&#39;s a normalization factor that comes from the \(\sqrt{2\pi}\) in the pdf</p>
\[
f_{\text{Normal}}(x|μ,σ) = \frac{1}{σ \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\ \ .
\]
<p>The thing is, we often don&#39;t need to normalize&#33; For MCMC, we usually only know the log-density up to addition of a constant anyway. So any time spent computing this, however small, is a waste.</p>
<p>But sometimes we <em>do</em> need it&#33; So it seems silly to just discard this. What we really need is to have</p>
<ul>
<li><p>No extra cost for computing the normalization, but</p>
</li>
<li><p>An easy way to recover it.</p>
</li>
</ul>
<h2 id="the_normal_measure_in_four_lines_of_code"><a href="#the_normal_measure_in_four_lines_of_code" class="header-anchor">The Normal Measure, in Four Lines of Code</a></h2>
<p>Here&#39;s the implementation in MeasureTheory.jl:</p>
<pre><code class="language-julia">@measure Normal&#40;μ,σ&#41; ≪ &#40;1/sqrt2π&#41; * Lebesgue&#40;ℝ&#41;

logdensity&#40;d::Normal&#123;&#40;&#41;&#125; , x&#41; &#61; - x^2 / 2 

Base.rand&#40;rng::Random.AbstractRNG, T::Type, d::Normal&#123;&#40;&#41;&#125;&#41; &#61; randn&#40;rng, T&#41;

@μσ_methods Normal&#40;&#41;</code></pre>
<p>Let&#39;s take these one by one.</p>
<h3 id="measure_normalμσ_1sqrt2π_lebesgueℝ"><a href="#measure_normalμσ_1sqrt2π_lebesgueℝ" class="header-anchor"><code>@measure Normal&#40;μ,σ&#41; ≪ &#40;1/sqrt2π&#41; * Lebesgue&#40;ℝ&#41;</code></a></h3>
<p>This can be read roughly as</p>
<blockquote>
<p><em>Define a new parameterized measure called <code>Normal</code> with default parameters <code>μ</code> and <code>σ</code>. The &#40;log-&#41;density will be defined later, and will have a normalization factor of \(\frac{1}{\sqrt{2\pi}}\) with respect to Lebesgue measure on \(\mathbb{R}\).</em></p>
</blockquote>
<p>So then for example,</p>
<pre><code class="language-julia">using MeasureTheory
basemeasure&#40;Normal&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">0.398942 * Lebesgue(ℝ)</code></pre>
<p>and</p>
<pre><code class="language-julia">Normal&#40;&#41; ≪ Lebesgue&#40;ℝ&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<h3 id="logdensitydnormal_x_-_x2_2"><a href="#logdensitydnormal_x_-_x2_2" class="header-anchor"><code>logdensity&#40;d::Normal&#123;&#40;&#41;&#125; , x&#41; &#61; - x^2 / 2</code></a></h3>
<p>This defines the log-density relative to the base measure. There&#39;s no <code>μ</code> or <code>σ</code> yet, this is just for a standard normal.</p>
<p>If we really wanted this in &quot;the usual way&quot; &#40;that is, with respect to Lebesgue measure&#41;, there&#39;s a three-argument form that lets us specify the base measure:</p>
<pre><code class="language-julia">logdensity&#40;Normal&#40;&#41;, Lebesgue&#40;ℝ&#41;, 1.0&#41;</code></pre><pre><code class="plaintext code-output">-1.4189385332046727</code></pre>
<p>which ought to match up with</p>
<pre><code class="language-julia">Dists.logpdf&#40;Dists.Normal&#40;&#41;, 1.0&#41;</code></pre><pre><code class="plaintext code-output">-1.4189385332046727</code></pre>
<p>By the way, <code>MeasureTheory</code> exports <code>Distributions</code> as <code>Dists</code> for easy typing. And yes, exery <code>Dists.Distribution</code> can be used from MeasureTheory; you&#39;ll just miss out on some flexibility and performance.</p>
<h3 id="baserandrngrandomabstractrng_ttype_dnormal_randnrng_t"><a href="#baserandrngrandomabstractrng_ttype_dnormal_randnrng_t" class="header-anchor"><code>Base.rand&#40;rng::Random.AbstractRNG, T::Type, d::Normal&#123;&#40;&#41;&#125;&#41; &#61; randn&#40;rng, T&#41;</code></a></h3>
<p>Hopefully this one is clear. One fine point worth mentioning is that some dispatch happening outside of this gives us some methods for free. So for example, we could do</p>
<pre><code class="language-julia">rng &#61; Random.GLOBAL_RNG
rand&#40;rng, Float32, Normal&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">-1.523651f0</code></pre>
<p>but also</p>
<pre><code class="language-julia">rand&#40;rng, Normal&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">0.22558175089484894</code></pre>
<p>or</p>
<pre><code class="language-julia">rand&#40;Float16, Normal&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">Float16(0.631)</code></pre>
<p>or just</p>
<pre><code class="language-julia">rand&#40;Normal&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">1.566945776473056</code></pre>
<h3 id="μσ_methods_normal"><a href="#μσ_methods_normal" class="header-anchor"><code>@μσ_methods Normal&#40;&#41;</code></a></h3>
<p>This is our last line, and we still haven&#39;t touched anything outside a standard normal &#40;mean zero, standard deviation one&#41;.</p>
<p>But there&#39;s nothing about this last step that&#39;s specific to normal distributions; it&#39;s just one example of a <a href="https://en.wikipedia.org/wiki/Location&#37;E2&#37;80&#37;93scale_family">location-scale family</a>. There&#39;s so much common behavior here across distributions, we should be able to abstract it.</p>
<p>So that&#39;s what we do&#33; Here&#39;s the code generated by <code>@μσ_methods Normal&#40;&#41;</code>:</p>
<pre><code class="language-julia">function Base.rand&#40;rng::AbstractRNG, T::Type, d::Normal&#123;&#40;:μ, :σ&#41;&#125;&#41;
    d.σ * rand&#40;rng, T, Normal&#40;&#41;&#41; &#43; d.μ
end

function logdensity&#40;d::Normal&#123;&#40;:μ, :σ&#41;&#125;, x&#41;
    z &#61; &#40;x - d.μ&#41; / d.σ
    return logdensity&#40;Normal&#40;&#41;, z&#41; - log&#40;d.σ&#41;
end

function Base.rand&#40;rng::AbstractRNG, T::Type, d::Normal&#123;&#40;:σ,&#41;&#125;&#41;
    d.σ * rand&#40;rng, T, Normal&#40;&#41;&#41;
end

function logdensity&#40;d::Normal&#123;&#40;:σ,&#41;&#125;, x&#41;
    z &#61; x / d.σ
    return logdensity&#40;Normal&#40;&#41;, z&#41; - log&#40;d.σ&#41;
end

function Base.rand&#40;rng::AbstractRNG, T::Type, d::Normal&#123;&#40;:μ,&#41;&#125;&#41;
    rand&#40;rng, T, Normal&#40;&#41;&#41; &#43; d.μ
end

function logdensity&#40;d::Normal&#123;&#40;:μ,&#41;&#125;, x&#41;
    z &#61; x - d.μ
    return logdensity&#40;Normal&#40;&#41;, z&#41;
end</code></pre>
<p>Using this approach, we have just one family of measures call <code>Normal</code>, but several parameterizations. So far we have four:</p>
<ul>
<li><p><code>Normal&#123;&#40;&#41;&#125;</code></p>
</li>
<li><p><code>Normal&#123;&#40;:μ,&#41;&#125;</code></p>
</li>
<li><p><code>Normal&#123;&#40;:σ,&#41;&#125;</code></p>
</li>
<li><p><code>Normal&#123;&#40;:μ, :σ&#41;&#125;</code></p>
</li>
</ul>
<p>And adding more is easy&#33; Here are a few that might be convenient, depending on the application:</p>
<ul>
<li><p><code>Normal&#123;&#40;:μ, :σ²&#41;&#125;</code> &#40;mean and variance&#41;</p>
</li>
<li><p><code>Normal&#123;&#40;:μ, :τ&#41;&#125;</code> &#40;mean and inverse variance, also called <em>precision</em>&#41;</p>
</li>
<li><p><code>Normal&#123;&#40;:μ, :logσ&#41;&#125;</code> &#40;mean and log-standard-deviation, sometimes useful for MCMC&#41;</p>
</li>
<li><p><code>Normal&#123;&#40;:q₀₁, :q₉₉&#41;&#125;</code> &#40;quantiles&#41;</p>
</li>
</ul>
<h2 id="performance"><a href="#performance" class="header-anchor">Performance</a></h2>
<p>Let&#39;s set up a little benchmark. Given some arrays</p>
<pre><code class="language-julia">μ &#61; randn&#40;1000&#41; 
σ &#61; rand&#40;1000&#41;
x &#61; randn&#40;1000&#41;
y &#61; randn&#40;1000&#41;</code></pre>
<p>For each <code>i</code>, we&#39;ll</p>
<ol>
<li><p>Build a <code>Normal&#40;μ&#91;i&#93;, σ&#91;i&#93;&#41;</code></p>
</li>
<li><p>Evaluate the log-density of this at <code>x&#91;i&#93;</code></p>
</li>
<li><p>Store the result in <code>y&#91;i&#93;</code></p>
</li>
</ol>
<p>We should expect big time differences for this versus a standard normal, so let&#39;s also measure this with fixed <code>μ</code>, fixed <code>σ</code>, and with both fixed. The only thing we really need to vary is the way we build the distribution or measure. So we can do this:</p>
<pre><code class="language-julia">using BenchmarkTools

function array_work&#40;f, μ, σ, x, y&#41;
    @inbounds for i in eachindex&#40;x&#41;
        y&#91;i&#93; &#61; logdensity&#40;f&#40;μ&#91;i&#93;, σ&#91;i&#93;&#41;, x&#91;i&#93;&#41;
    end 
end

time_normal&#40;f&#41; &#61; @belapsed &#36;array_work&#40;&#36;f, &#36;μ, &#36;σ, &#36;x, &#36;y&#41;</code></pre>
<p>The we can get the timings like this:</p>
<pre><code class="language-julia">mμσ &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Normal&#40;μ,σ&#41;&#41; 
dμσ &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Dists.Normal&#40;μ,σ; check_args&#61;false&#41;&#41;

mμ1 &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Normal&#40;μ&#61;μ&#41;&#41; 
dμ1 &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Dists.Normal&#40;μ&#41;&#41;

m0σ &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Normal&#40;σ&#61;σ&#41;&#41; 
d0σ &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Dists.Normal&#40;0.0,σ; check_args&#61;false&#41;&#41;

m01 &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Normal&#40;&#41;&#41;
d01 &#61; time_normal&#40;&#40;μ,σ&#41; -&gt; Dists.Normal&#40;&#41;&#41;</code></pre>
<p>Note here that there are a few different ways of calling this &#40;thanks to David Widmann for pointing out the <code>Dists.Normal&#40;μ&#41;</code> method&#41;. Also, as mentioned above, the <code>check_args</code> keyword argument makes things a little faster in some cases, but throws an error in others.</p>
<p>Finally, a plot:</p>
<pre><code class="language-julia">using StatsPlots

times_d &#61; &#91;dμσ, dμ1, d0σ, d01&#93;
times_m &#61; &#91;mμσ, mμ1, m0σ, m01&#93;

times &#61; &#91;times_d ;times_m&#93; / 1e3 * 1e9

pkg &#61; repeat&#40;&#91;&quot;Distributions.jl&quot;, &quot;MeasureTheory.jl&quot;&#93;, inner&#61;4&#41;
dist &#61; repeat&#40;&#91;&quot;Normal&#40;μ,σ&#41;&quot;, &quot;Normal&#40;μ,1&#41;&quot;, &quot;Normal&#40;0,σ&#41;&quot;, &quot;Normal&#40;0,1&#41;&quot;&#93;, outer&#61;2&#41;
groupedbar&#40;dist, times, group&#61;pkg, legend&#61;:topleft&#41;
ylabel&#33;&#40;&quot;Time per element &#40;ns&#41;&quot;&#41;</code></pre>
<img src="/assets/blog/2021/01-28-measure-theory/code/output/dists-measuretheory-times.svg" alt="">
<p>To be clear, Distributions is doing a little more work here, since it&#39;s including the normalization constant at each step. But that&#39;s exactly the point&#33; For many computations like MCMC, there&#39;s no need to do this. Also, we&#39;re not really throwing away this constant; we can recover it later if we like by asking for the log-density with respect to Lebesgue measure.</p>
<p>If Distributions had a way to do this without including the normalization, that might be a more fair comparison. But it doesn&#39;t, so if you&#39;re choosing between Distributions and MeasureTheory for MCMC, the plot above is a reasonable representation of the core log-density computation.</p>
<p>Also worth noting is that gradient computations are often important for this work. MeasureTheory is designed to be relatively autodiff-friendly, by representing the log-density as a simple algebraic expression. For Distributions this is definitely not the case, and making AD work well required an entirely separate and significant effort, <a href="https://github.com/TuringLang/DistributionsAD.jl">DistributionsAD.jl</a>.</p>
<h2 id="final_notes"><a href="#final_notes" class="header-anchor">Final Notes</a></h2>
<p>We&#39;ve really only scratched the surface of MeasureTheory.jl. There&#39;s also</p>
<ul>
<li><p>Multiple parameterizations for a given measure</p>
</li>
<li><p>Using measures for &quot;improper priors&quot;</p>
</li>
<li><p>Radon-Nikodym derivatives</p>
</li>
<li><p>Singular measures, like spike and slab priors</p>
</li>
<li><p>Markov kernels</p>
</li>
</ul>
<p>The library is still changing quickly, and we&#39;d love to have more community involvement. Please check it out&#33;</p>
<p><a href="https://github.com/cscherrer/MeasureTheory.jl">https://github.com/cscherrer/MeasureTheory.jl</a></p>
<div class="page-foot">
  <div class="copyright">
    &copy; Chad Scherrer. Last modified: November 27, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->

      </div> <!-- closure of main -->
    </div>   <!-- closure of class initial--content -->

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
        <!-- end custom footer snippets -->
        <div class="page__footer-follow">
          <ul class="social-icons">
            <li><strong>Follow:</strong></li>
            <li><a href="https://twitter.com/ChadScherrer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
            <li><a href="https://github.com/cscherrer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          </ul>
        </div>
        <div class="page__footer-copyright">&copy; 2021 Chad Scherrer. Powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="/libs/minimal-mistakes/main.min.js"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>

    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
